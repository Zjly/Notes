# 一周工作简报

### 7.5-7.11

##### 本周

1.阅读《Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding》，该论文提出了自然语言word与API在语义上不能有效地匹配的问题，提出了一种名为word2API的方法。该方法从大规模的语料库上获取大量的API注释以及API序列信息，从中构建word-API元组获取到word与API之间的潜在关系，并随机打乱word与API增加它们之间的搭配，最后使用词嵌入技术将它们映射到同一个向量空间中，将word与API之间构造了低维的表示并体现语义相关性。

2.阅读《Holistic Combination of Structural and Textual Code Information for Context based API Recommendation》，该论文提出了一种名为APIRec-CST的API推荐方法，解决了结构和文本代码独立建模的局限性以及对代码结构缺乏整体推理的问题。基于Code Token Network和API Context Graph Network来结合API流程图和文本代码信息。使用上下文图为整个方法的控制和数据流图中的API使用建模，并从中学习提取用于API推荐的信息结构特征。最后将代码的结构信息和文本信息联合在一起，进行API的推荐。

##### 下周

1.阅读《Next Item Recommendation with Self-Attention》

2.编写代码，在API序列推荐中加入流行度信息，对API序列的多样化进行推荐，解决长尾效应和序列相似度过高的问题。

### 7.12-7.18

##### 本周

1.阅读论文《Next Item Recommendation with Self-Attention》，论文提出了一种基于自注意力机制的度量学习方法来用于顺序推荐，模型结合了用户的短期意图和长期偏好来预测用户的下一步行动，利用了自注意力机制从用户最近的行为中学习用户的短期意图，模型可以准确地捕捉用户最近操作的重要性。其论文模型由两部分所组成：模拟用户的短期意图的*self-attention module*和模拟用户长期偏好的*collaborative metric learning*组件。此外，论文将自注意力机制推广到了序列预测任务的度量学习方法中，该方法在序列推荐中具有良好的效果。论文使用自适应梯度算法优化了所提出的方法，该算法可以自动适应步长。在推荐阶段，候选项目根据公式计算的推荐分数按升序排列，并将排名靠前的项目推荐给用户。模型的架构包含了用户的短暂的意图和长期的偏好，通过两者相加已生成最终的推荐列表。前者是通过自注意力网络从最近的交互操作中推断出来的，整个系统是在度量学习的框架下构建的。

2.阅读并理解tag数据集生成的部分代码。

##### 下周

1.阅读《Unified Language Model Pre-training for Natural Language Understanding and Generation》

2.继续阅读理解tag数据集生成的代码。

### 7.19-7.25

##### 本周

1.阅读论文《Unified Language Model Pre-training for Natural Language Understanding and Generation》，论文提出了一种新的统一预训练语言模型UNILM，可以针对自然语言理解和生成任务进行微调。与BERT类似，预先训练的UNILM可以进行微调以适应各种下游任务。但与主要用于自然语言处理任务的BERT不同，UNILM可以使用不同的自注意力掩码为不同类型的LM聚合上下文，因此可以用于自然语言处理和自然语言处理任务。UNILM基于Transformer模型，使用自注意力机制对token进行编码。在不同的LM任务上，UNILM使用了不同的mask矩阵来调整当前token所能注意到的范围：单向LM仅能注意到当前位置以及当前位置之前的tokens；双向LM能注意到整个句子中的所有tokens；seq2seqLM中句子被分为源序列和目标序列，源序列中token仅能注意到源序列中的所有tokens，目标序列中token仅能注意到源序列中的所有tokens以及目标序列中当前token以及其之前的tokens。UNILM使用不同的mask矩阵来适应多种不同的任务，并对于多个LM目标进行了联合优化，它们之间使用了共享的参数和架构以减轻了不同类型的LM的训练需要。论文提出的UNILM有三个主要优点：统一预训练程序生成了一个单一的Transformer，其为不同类型的LM使用共享的参数和架构，减轻了不同类型的LM的训练需要；参数共享使得学习到的文本表示更为的通用，因为它们针对了不同的语言建模目标进行了联合优化，上下文以不同的方式使用，减轻了对单个LM任务的过度拟合；UNILM可以作为seq2seq的LM来使用。UNILM相比已有的模型，在自然语言生成的任务性能上得到了有效的提升，可以作为一种更先进的seq2seq模型用于文本的生成。

[1] Dong L ,  Yang N ,  Wang W , et al. Unified Language Model Pre-training for Natural Language Understanding and Generation[J].  2019.

2.阅读理解tag数据集生成的代码。

3.撰写部分《面向社交网络的数据安全和隐私保护场景报告》文档。

##### 下周

1.阅读论文《Deep Session Interest Network for Click-Through Rate Prediction》

[1] Feng Y ,  Lv F ,  Shen W , et al. Deep Session Interest Network for Click-Through Rate Prediction[C]// 2019.

2.继续撰写《面向社交网络的数据安全和隐私保护场景报告》文档

### 7.26-8.2

##### 本周

1.阅读论文《Deep Session Interest Network for Click-Through Rate Prediction》，论文指出了用户的连续行为会由多个历史会话组成，并且用户行为在同会话中同构，在跨会话中异构。在此基础上，论文提出了深度会话兴趣网络(DSIN)：首先利用带有偏差编码的自注意力机制提取用户对每个会话的兴趣，然后使用Bi-LSTM来捕获上下文会话兴趣的顺序关系，最后利用局部激活单元聚合用户对目标项的不同会话兴趣表示。

2.完成撰写《面向社交网络的数据安全和隐私保护场景报告》文档。

##### 下周

1.阅读论文《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》
